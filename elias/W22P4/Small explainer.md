![logo](https://eliasdh.com/assets/media/images/logo-github.png)
# ğŸ’™ğŸ¤Small explainerğŸ¤ğŸ’™

## ğŸ“˜Table of Contents

1. [ğŸ“˜Table of Contents](#ğŸ“˜table-of-contents)
2. [ğŸ––Introduction](#ğŸ––introduction)
3. [ğŸ“ŠDropout Layer](#ğŸ“Šdropout-layer)
    1. [ğŸPython Example:](#ğŸpython-example)
4. [ğŸ“ŠBatch Normalization Layer](#ğŸ“Šbatch-normalization-layer)
    1. [ğŸPython Example:](#ğŸpython-example)
5. [ğŸ“ŠActivation Layer](#ğŸ“Šactivation-layer)
    1. [ğŸPython Example:](#ğŸpython-example)
6. [ğŸ“ŠLeakyReLU Layer](#ğŸ“Šleakyrelu-layer)
    1. [ğŸPython Example:](#ğŸpython-example)
7. [ğŸ“ŠGaussian Noise Layer](#ğŸ“Šgaussian-noise-layer)
    1. [ğŸPython Example:](#ğŸpython-example)
8. [ğŸ“ŠGaussian Dropout Layer](#ğŸ“Šgaussian-dropout-layer)
    1. [ğŸPython Example:](#ğŸpython-example)
9. [ğŸ“ŠELU (Exponential Linear Unit) Layer](#ğŸ“Šelu-exponential-linear-unit-layer)
    1. [ğŸPython Example:](#ğŸpython-example)
10. [ğŸ“ŠExample of Incorporating These Layers](#ğŸ“Šexample-of-incorporating-these-layers)
    1. [ğŸPython Example:](#ğŸpython-example)
11. [ğŸ”—Links](#ğŸ”—links)

---

## ğŸ––Introduction

This document provides a brief overview of some common probability distributions and how to generate random numbers from them using Python.

## ğŸ“ŠDropout Layer

Dropout layers help prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time.

### ğŸPython Example:

```python
from keras.layers import Dropout

x_iris_model = Dropout(0.5)(x_iris_model)
```

## ğŸ“ŠBatch Normalization Layer

Batch normalization layers normalize the output of the previous layers to speed up training and provide some regularization.

### ğŸPython Example:

```python
from keras.layers import BatchNormalization

x_iris_model = BatchNormalization()(x_iris_model)
```

## ğŸ“ŠActivation Layer

Instead of specifying the activation function within the Dense layer, you can use a separate Activation layer for more complex activation functions.

### ğŸPython Example:

```python
from keras.layers import Activation

x_iris_model = Activation('relu')(x_iris_model)
```

## ğŸ“ŠLeakyReLU Layer

LeakyReLU is an alternative to standard ReLU activation that allows a small gradient when the unit is not active, which can help with the "dying ReLU" problem.

### ğŸPython Example:

```python
from keras.layers import LeakyReLU

x_iris_model = LeakyReLU(alpha=0.1)(x_iris_model)
```

## ğŸ“ŠGaussian Noise Layer

This layer adds Gaussian noise to the input, which can help with regularization.

### ğŸPython Example:

```python
from keras.layers import GaussianNoise

x_iris_model = GaussianNoise(0.1)(x_iris_model)
```

## ğŸ“ŠGaussian Dropout Layer

This layer applies multiplicative 1-centered Gaussian noise.

### ğŸPython Example:

```python
from keras.layers import GaussianDropout

x_iris_model = GaussianDropout(0.5)(x_iris_model)
```

## ğŸ“ŠELU (Exponential Linear Unit) Layer

TELU is an alternative activation function that can help with the vanishing gradient problem.

### ğŸPython Example:

```python
from keras.layers import ELU

x_iris_model = ELU(alpha=1.0)(x_iris_model)
```

## ğŸ“ŠExample of Incorporating These Layers

Here's an example of how you might integrate some of these layers into your model:

### ğŸPython Example:

```python
from keras.models import Model
from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation, LeakyReLU, GaussianNoise, ELU
from keras.optimizers import Adam

inputs_iris = Input(shape=(4,))
x_iris_model = Dense(8)(inputs_iris)
x_iris_model = BatchNormalization()(x_iris_model)
x_iris_model = LeakyReLU(alpha=0.1)(x_iris_model)
x_iris_model = Dense(16)(x_iris_model)
x_iris_model = ELU(alpha=1.0)(x_iris_model)
x_iris_model = Dropout(0.5)(x_iris_model)
x_iris_model = Dense(32)(x_iris_model)
x_iris_model = GaussianNoise(0.1)(x_iris_model)
x_iris_model = Activation('sigmoid')(x_iris_model)
x_iris_model = Dense(16)(x_iris_model)
x_iris_model = Activation('sigmoid')(x_iris_model)
x_iris_model = Dense(8)(x_iris_model)
x_iris_model = Activation('sigmoid')(x_iris_model)
outputs_iris = Dense(3, activation='softmax')(x_iris_model)
model_iris = Model(inputs_iris, outputs_iris, name='Iris_NN')

model_iris.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming x_iris_learning_normalize and y_iris_learning_replace are already defined
history_iris = model_iris.fit(
    x_iris_learning_normalize,
    y_iris_learning_replace,
    epochs=1000,
    callbacks=[PlotLossesKeras()],
    verbose=False
)
```

## ğŸ”—Links
- ğŸ‘¯ Web hosting company [EliasDH.com](https://eliasdh.com).
- ğŸ“« How to reach us elias.dehondt@outlook.com.